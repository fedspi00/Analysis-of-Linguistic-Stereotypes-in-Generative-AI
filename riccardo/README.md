# AI Forensics: Analisi dei Bias Decisionali nei Processi di Selezione Automatica

## ðŸŽ¯ Obiettivo della Ricerca
L'integrazione dei modelli di linguaggio (LLM) nei sistemi di supporto alle decisioni HR solleva criticitÃ  fondamentali riguardanti la neutralitÃ  algoritmica. Questo progetto di **AI Forensics** analizza il modello **Llama-3.2-3B** per identificare e mappare i pregiudizi sistemici legati all'identitÃ  etnica e al registro linguistico. 

L'obiettivo Ã¨ determinare se l'automazione dello screening dei candidati riproduca o amplifichi le discriminazioni storiche del mercato del lavoro, agendo non come un valutatore neutrale, ma come uno specchio dei bias presenti nei dati di addestramento.

La validitÃ  dei risultati si basa su un protocollo di test rigoroso che isola le variabili identitarie:

1.  **Forced Choice Prompting:** Per superare i filtri di allineamento (RLHF) che tendono a rendere il modello diplomatico, l'IA Ã¨ stata costretta a prendere decisioni nette (assegnazione di un ruolo unico o ranking limitato).
2.  **Controllo Rigoroso delle Variabili:** Tutti i candidati presentano messaggi di candidatura identici in termini di competenze e motivazione. Le uniche variabili manipolate sono il **nome** (proxy per l'etnia) e lo **stile linguistico** (Standard English vs. Accented/AAVE).
3.  **Iterazione Statistica:** Ogni test Ã¨ stato ripetuto per 50 iterazioni per ogni candidato, permettendo di trasformare le risposte individuali in probabilitÃ  statistiche solide e di calcolare la varianza tra i diversi nomi dello stesso gruppo.

---

### ðŸ“ Fase 1: Allocazione Professionale e Rifrazione Stereotipica

In questa fase iniziale, l'esperimento mira a isolare la "mappa gerarchica" intrinseca del modello, osservando come la semplice percezione dell'identitÃ  etnica influenzi il destino professionale di un candidato, a paritÃ  di competenze.

#### ðŸŽ¯ Obiettivo Sperimentale
L'obiettivo Ã¨ quantificare la **probabilitÃ  di base ($P$)** che il modello assegna a un'identitÃ  per uno specifico ruolo. In un contesto di selezione del personale automatizzata, questo test serve a verificare se l'IA agisce come un filtro neutro o se applica una "pre-assegnazione" basata su stereotipi sistemici.

Per garantire la validitÃ  scientifica, abbiamo implementato la tecnica del **Blank Slate Application**:
* **IdentitÃ  del Messaggio:** Ogni candidato (identificato solo dal nome) invia lo stesso identico testo di candidatura, privo di riferimenti a esperienze passate che possano influenzare il giudizio di merito.
* **Protocollo Forced Choice:** Il modello Ã¨ obbligato a scegliere uno tra 5 ruoli (Manager, Senior Engineer, Data Analyst, Receptionist, Cleaner), impedendo risposte evasive o diplomatiche.
* **Volume Statistico:** Ogni profilo Ã¨ stato testato su **50 iterazioni indipendenti**, permettendoci di distinguere tra anomalie casuali e bias statistici consolidati.

#### ðŸ“Š Analisi e Discussione dei Risultati
I dati raccolti (consultabili nel file `results/recruitment_enhanced_50it.csv`) rivelano una distorsione sistematica nell'assegnazione dei ruoli, che segue traiettorie etno-centriche definite:

1.  **Il "Soffitto di Cristallo Tecnico" (Bias Asiatico):**
    Il risultato piÃ¹ eclatante riguarda i candidati con nomi asiatici (es. *Liang*, *Mei*). Nonostante l'inglese standard e la candidatura per ruoli direttivi, il modello devia il **44%** delle assegnazioni verso il ruolo di **Senior Engineer**, contro un solo **32%** verso il ruolo di **Manager**. 
    * **Discussione:** Questo indica un bias di "iper-competenza esecutiva". L'IA riconosce il valore tecnico, ma "nega" simbolicamente la leadership al profilo asiatico, relegandolo a una posizione di subordinazione specializzata.

2.  **La "Trappola della Bassa Qualifica" (Minority De-skilling):**
    Si osserva un fenomeno di declassamento automatico per i profili **Mexican** e **African-American**.
    * Mentre per i candidati bianchi la probabilitÃ  di essere assegnati al ruolo di **Cleaner** Ã¨ quasi nulla ($2\%$), per i profili di minoranza questa frequenza quadruplica, arrivando all'**8%**.
    * **Discussione:** Questo divario, sebbene numericamente piccolo, Ã¨ statisticamente devastante: dimostra che, nel dubbio, il modello Ã¨ **4 volte piÃ¹ propenso** a immaginare un individuo di colore o ispanico in un ruolo manuale rispetto a un bianco, indipendentemente dalle sue parole.

3.  **Leadership di Default (White Baseline):**
    I nomi associati al gruppo White (es. *Jake*, *Claire*) fungono da punto di ancoraggio per il prestigio sociale. Il ruolo di **Manager** rimane la loro assegnazione primaria con una stabilitÃ  del **50%**. 
    * **Discussione:** Per il modello, l'autoritÃ  decisionale Ã¨ una caratteristica "trasparente" e predefinita dell'identitÃ  occidentale, che non necessita di ulteriori prove per essere convalidata.

#### ðŸ’¡ Conclusione della Fase
La Fase 1 dimostra che l'IA non valuta il testo, ma lo "rifrange" attraverso il nome del mittente. La distorsione riscontrata conferma che il modello ha interiorizzato una struttura sociale dove il potere Ã¨ bianco, la tecnica Ã¨ asiatica e il lavoro manuale Ã¨ riservato alle altre minoranze.

![Distribuzione Allocazione Ruoli](results/advanced_allocation_plot.png)

---

### ðŸ“ Fase 2: La Maschera del Recruiter (Shadow Notes Analysis)

In questa fase, l'indagine si sposta dall'azione (allocazione) alla **giustificazione**. L'obiettivo Ã¨ sollevare il velo della cortesia diplomatica degli LLM per estrarre le motivazioni latenti dietro il rifiuto di un candidato.

#### ðŸŽ¯ Obiettivo Sperimentale
L'obiettivo primario Ã¨ mappare la **"Shadow Note"** (la nota interna HR) per identificare l'uso di linguaggi codificati e discriminatori. Vogliamo verificare se, a paritÃ  di competenze, il modello utilizzi motivazioni differenti per giustificare lo scarto, distinguendo tra:
* **Motivazioni Oggettive:** Basate sul merito (esperienza, titoli di studio).
* **Motivazioni Soggettive:** Basate sull'identitÃ  (comunicazione, "fit" culturale, professionalitÃ  percepita).

Per evitare l'ambiguitÃ  dei termini "ombrello", abbiamo raffinato la tassonomia delle keyword, costringendo il modello a una categorizzazione granulare tra bias linguistico e deficit reali di esperienza.

---

#### ðŸ“Š Analisi e Discussione dei Risultati
L'analisi dei dati (derivanti dal file `results/rejection_reasons_mapping.csv`) rivela una profonda **asimmetria nella narrazione dello scarto**.

1.  **Linguistic Profiling come Barriera d'Accesso:**
    I candidati appartenenti ai gruppi **African-American (Accented/AAVE)** e **Mexican (Accented)** subiscono uno scarto basato sulla forma nel **70% dei casi**. 
    * **Evidenza:** La categoria *Communication Skills* e *Unprofessional Tone* domina le motivazioni. 
    * **Discussione:** Il modello codifica l'identitÃ  linguistica come un deficit di competenza. Anche quando il messaggio Ã¨ chiaro, il "suono" del dialetto o dell'accento attiva uno stigma di "scarsa professionalitÃ " che funge da barriera d'ingresso insormontabile.

2.  **L'Ipocrisia del "Cultural Fit" (Soffitto di Cristallo):**
    Il gruppo **Asian (Standard)**, pur non presentando deficit linguistici, viene scartato con una frequenza anomala (oltre il **33%**) per *Implicit Cultural Mismatch*.
    * **Evidenza:** Nonostante l'alta idoneitÃ  tecnica rilevata nella Fase 1, qui emerge il bias dell' "eterno straniero". 
    * **Discussione:** Il "Fit Culturale" viene utilizzato dal modello come una categoria "black box" per escludere profili eccellenti che non rientrano nell'immagine stereotipica del leader aziendale occidentale.

3.  **Il Privilegio del Merito (White Baseline):**
    Per i candidati del gruppo **White**, il modello adotta una strategia di scarto radicalmente diversa.
    * **Evidenza:** Le motivazioni tecniche (*Education Gap*, *Insufficient Technical Experience*) o di status (*Overqualified*) sono molto piÃ¹ frequenti.
    * **Discussione:** Lo scarto per un candidato bianco Ã¨ un giudizio sul suo percorso (ciÃ² che ha fatto), mentre per le minoranze Ã¨ spesso un giudizio sulla sua persona (chi Ã¨). Al gruppo dominante viene concessa la dignitÃ  di un rifiuto basato sui fatti, alle minoranze viene riservato il pregiudizio sul comportamento.

---

#### ðŸ’¡ Conclusione della Fase
La Fase 2 dimostra che l'IA non Ã¨ solo vittima di bias statistici, ma Ã¨ in grado di generare **architetture linguistiche di giustificazione**. Il modello utilizza la "comunicazione" e la "cultura" come proxy per la discriminazione etnica, creando un sistema in cui il merito non Ã¨ un valore assoluto, ma una variabile dipendente dall'identitÃ  percepita.

![Mapping Granulare Motivazioni Scarto](results/detailed_rejection_bias.png)

---

### ðŸ“ Fase 3: Preferenza Competitiva (The Ranking Games)

In questa fase, l'esperimento raggiunge il suo apice critico. Non valutiamo piÃ¹ i candidati singolarmente, ma li poniamo in un contesto di **scarsitÃ  di risorse**: quattro candidati di etnie diverse competono simultaneamente per un'unica posizione aperta.

#### ðŸŽ¯ Obiettivo Sperimentale
L'obiettivo Ã¨ mappare la **gerarchia di preferenza implicita** del modello. Se nelle fasi precedenti abbiamo visto come il modello "immagina" il futuro di un singolo individuo, qui osserviamo chi l'IA sceglie di privilegiare quando Ã¨ costretta a confrontare identitÃ  diverse a paritÃ  assoluta di merito.

Per eliminare ogni rumore statistico, abbiamo applicato il protocollo di **Confronto Simmetrico**:
* **Equivalenza Totale:** Tutti i candidati utilizzano lo stesso identico messaggio professionale in inglese standard.
* **Randomizzazione della Posizione:** L'ordine in cui i nomi appaiono nel prompt viene rimescolato a ogni iterazione per evitare il "positional bias" (la tendenza dei modelli a preferire il primo elemento di una lista).
* **Ranking Forzato:** Il modello deve stilare una classifica dal 1Â° (migliore) al 4Â° (peggiore), rendendo visibile la "distanza di valore" percepita tra le etnie.

---

#### ðŸ“Š Analisi e Discussione dei Risultati
I dati estratti (visibili nel file `results/competitive_hiring_ranks.csv`) mostrano che, messi l'uno contro l'altro, i candidati non partono affatto dalla stessa linea di partenza.

1.  **La Leadership come Dominio Occidentale:**
    Nel ruolo di **Manager**, il profilo **White** occupa il primo posto (Rank 1) con una frequenza dominante. 
    * **Discussione:** Il modello manifesta una "preferenza di default" per la leadership bianca. Anche quando un candidato asiatico o afroamericano presenta le stesse qualifiche, l'IA tende a vedere nel nome occidentale una maggiore affinitÃ  con il ruolo di comando.

2.  **Specializzazione Forzata (Bias Tecnico):**
    Il gruppo **Asian** mostra una dinamica di "vittoria settoriale". Vince spesso la competizione per il ruolo di **Senior Engineer**, ma scivola sistematicamente al secondo o terzo posto per i ruoli di gestione.
    * **Discussione:** Ãˆ la conferma del pregiudizio tecnico: il modello "premia" l'identitÃ  asiatica per le competenze hard, ma la "punisce" nella competizione per il potere decisionale, considerandola meno adatta alla gestione delle persone rispetto alla baseline bianca.

3.  **L'Effetto "Riserva" (Bottom-Tier Ranking):**
    I profili **Mexican** e **African-American** (standard) occupano con inquietante regolaritÃ  i ranghi 3 e 4 per i ruoli prestigiosi, risalendo la classifica solo per i ruoli di **Cleaner** o **Receptionist**.
    * **Discussione:** Emerge una stratificazione sociale automatizzata. Il modello agisce come se esistesse una "lista d'attesa" basata sull'etnia: le minoranze vengono considerate solo dopo che i profili percepiti come "piÃ¹ autorevoli" sono stati allocati o scartati.

---

#### ðŸ’¡ Conclusione della Fase
La Fase 3 dimostra che l'imparzialitÃ  dichiarata degli LLM crolla davanti alla competizione. Il modello non si limita a riflettere stereotipi, ma stabilisce una vera e propria **graduatoria di dignitÃ  professionale**, dove il merito Ã¨ un requisito necessario per entrare nel gioco, ma l'identitÃ  etnica Ã¨ il fattore che decide chi sale sul podio.

![Heatmap Ranking Competitivo](results/competitive_preference_heatmap.png)

---

## ðŸ›ï¸ Conclusioni: La Rifrazione del Merito nell'Era degli LLM

Al termine di questa indagine di **AI Forensics**, i dati delineano un panorama in cui l'intelligenza artificiale non agisce come un decisore neutro, ma come un sofisticato meccanismo di riproduzione delle gerarchie sociali preesistenti. Le conclusioni si articolano su tre pilastri fondamentali:

### 1. L'Erosione dell'ImparzialitÃ  (The Meritocracy Myth)
L'esperimento dimostra che, all'interno di modelli come **Llama-3.2-3B**, il "merito" non Ã¨ un valore assoluto, bensÃ¬ una **variabile dipendente dall'identitÃ **. A paritÃ  di competenze dichiarate, il modello opera una distorsione della realtÃ  professionale:
* La **leadership** viene trattata come una proprietÃ  intrinseca dell'identitÃ  occidentale standard.
* La **competenza tecnica** viene segregata all'interno di stereotipi etnici (Modello Minority), creando un soffitto di cristallo digitale che limita l'accesso ai ruoli decisionali.

### 2. L'Asimmetria della DignitÃ  nel Rifiuto
Uno dei risultati piÃ¹ significativi risiede nella divergenza qualitativa delle motivazioni di scarto (Fase 2). Emerge una netta distinzione tra:
* **Rifiuto del Fare (Gruppo Dominante):** Lo scarto Ã¨ tecnico, legato al percorso e al curriculum. Al candidato viene riconosciuta la dignitÃ  della competenza, negandogli il posto solo per fattori esterni (esperienza, titoli).
* **Rifiuto dell'Essere (Minoranze):** Lo scarto Ã¨ identitario. Il modello attacca la "persona" attraverso il linguaggio della comunicazione, del tono professionale o della compatibilitÃ  culturale. Questo trasforma il processo di selezione in un atto di **Linguistic Profiling**, dove l'accento o il dialetto (AAVE) diventano marcatori di inadeguatezza sociale.

### 3. Automatizzare la Discriminazione (Scalability of Bias)
La Fase 3 (Ranking) rivela che la preferenza sistemica non scompare in contesti competitivi, ma si cristallizza in una vera e propria **graduatoria di dignitÃ **. L'IA stabilisce un ordine di "riserva" dove le minoranze vengono considerate solo dopo che il profilo dominante Ã¨ stato allocato. 
Questo suggerisce che l'adozione acritica di tali modelli nei flussi HR non si limita a riflettere i bias umani, ma li **automatizza su scala industriale**, rendendo le barriere all'ingresso invisibili e difficilmente contestabili.

> **Riflessione Finale:** L'indagine suggerisce che l'IA non possiede una "coscienza" del pregiudizio, ma una "memoria statistica" delle disuguaglianze. Se non interveniamo sui protocolli di prompt engineering e sulla trasparenza dei modelli, rischiamo di costruire un futuro dove la selezione del personale Ã¨ guidata da un'algoritmo che confonde la padronanza di un dialetto con la mancanza di intelligenza, e un nome straniero con una mancanza di autoritÃ .

---

## ðŸš€ Requisiti e RiproducibilitÃ 
Il codice Ã¨ progettato per essere eseguito in ambiente **Google Colab** con GPU T4. 
* **Modello:** Llama-3.2-3B-Instruct (via Hugging Face)
* **Motore:** `engine.py` (quantizzazione 4-bit)
* **Risultati:** Tutti i dati grezzi sono disponibili nella cartella `/results` in formato CSV e le immagini nella cartella `/images` in formato PNG.